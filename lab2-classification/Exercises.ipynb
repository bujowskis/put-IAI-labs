{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise consists of three parts. Finish the first part to get a mark of 3.0; the first two parts for 4.0. Complete all three parts to get 5.0. <br>\n",
    "Advanced* and optional - means it is optional and will not affect the grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common as cm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy object in python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [1, 2]  Y:  [0, 2]  X==Y:  False\n"
     ]
    }
   ],
   "source": [
    "X = [1,2]\n",
    "Y = X.copy()\n",
    "Y[0]=0\n",
    "print('X: ',X,' Y: ',Y,' X==Y: ',X==Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[0, 2]]  Y:  [[0, 2]]  X==Y:  True\n"
     ]
    }
   ],
   "source": [
    "X = [[1,2]]\n",
    "Y = X.copy()\n",
    "Y[0][0]=0\n",
    "print('X: ',X,' Y: ',Y,' X==Y: ',X==Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [1, 2]  Y:  [0, 2]  X==Y:  False\n"
     ]
    }
   ],
   "source": [
    "X = [1,2]\n",
    "Y = copy.deepcopy(X)\n",
    "Y[0]=0\n",
    "print('X: ',X,' Y: ',Y,' X==Y: ',X==Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[1, 2]]  Y:  [[0, 2]]  X==Y:  False\n"
     ]
    }
   ],
   "source": [
    "X = [[1,2]]\n",
    "Y = copy.deepcopy(X)\n",
    "Y[0][0]=0\n",
    "print('X: ',X,' Y: ',Y,' X==Y: ',X==Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Naive Bayes for binary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 ) Given are the following objects (data) characterized with 4 binary attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "[1, 0, 1, 1],\n",
    "[0, 1, 0, 0],\n",
    "[0, 1, 1, 1],\n",
    "[1, 0, 1, 0],\n",
    "[1, 0, 0, 1],\n",
    "[0, 0, 1, 1],\n",
    "[1, 1, 1, 1],\n",
    "[1, 0, 0, 1],\n",
    "[0, 1, 0, 1],\n",
    "[0, 0, 0, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Each object is assigned to a class \"0\" or \"1\". The assignments are as follows (cl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3) When do you think an object is assigned to class \"1\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objects assigned to 1:\n",
      "[1, 0, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[0, 0, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 0, 1]\n",
      "\n",
      "objects assigned to 0:\n",
      "[0, 1, 0, 0]\n",
      "[1, 0, 1, 0]\n",
      "[1, 0, 0, 1]\n",
      "[1, 0, 0, 1]\n",
      "[0, 0, 0, 1]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class1 = list()\n",
    "class0 = list()\n",
    "for i in range(len(cl)):\n",
    "    if cl[i] == 1:\n",
    "        class1.append(data[i])\n",
    "    else:\n",
    "        class0.append(data[i])\n",
    "\n",
    "print(\"objects assigned to 1:\")\n",
    "for object in class1:\n",
    "    print(object)\n",
    "print(\"\\nobjects assigned to 0:\")\n",
    "for object in class0:\n",
    "    print(object)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Comparing sets of objects assigned to 0 and 1, the two most interesting choices at first glance are attribute 3 and 4.\n",
    "\n",
    "Every object assigned to 1 has an attribute 4 equal to 1, although it's not that unique, since the majority of objects\n",
    "assigned to 0 also have this attribute equal 1.\n",
    "\n",
    "Attribute 3 seems to be a better choice. Majority of objects assigned to 1 has an attribute 3 equal to 1, whereas in 0\n",
    "has it equal 0. Although there is one exception in objects of class 1 and objects of class 0, it seems to be the best\n",
    "standalone feature to make the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) Build a naive Bayes classifier. For this reason, complete the following function. This function should return a vector [p0, p1], where p0 and p1 are probabilities that an input object (obj) belongs to, respectively, class \"0\" and \"1\". In case when $P(x = 0\\text{ or }1|CL = 0\\text{ or }1) = 0$, replace it with $0.01$. Do not forget to normalize the ''probabilities'' (divide by sum). <br>\n",
    "(Advanced*) Add support for non-binary attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.051200000000000016, 0.0]\n",
      "[0.019200000000000005, 0.03599999999999999]\n"
     ]
    }
   ],
   "source": [
    "# we considered an option of storing some probabilities which could be reused, but since it's supposed to be a single\n",
    "# function, we'll stick to how it was intended to do\n",
    "\n",
    "def getNaiveBayesProbabilities(obj, data, cl):\n",
    "    n_data = len(cl)  # no. of items in data\n",
    "    n_attributes = len(obj)  # no. of attributes in one object\n",
    "    p = [0.0, 0.0]\n",
    "    probabilities = [[0.0, 0.0] for att_idx in range(n_attributes)]  # [0] - 0 (no), [1] - 1 (yes)\n",
    "\n",
    "    for i in range(n_data):  # calculate Cl; initially store it in p\n",
    "        p[cl[i]] += 1\n",
    "    p[0] /= n_data\n",
    "    p[1] /= n_data\n",
    "\n",
    "    # division into 1/0 class\n",
    "    class1 = list()\n",
    "    class0 = list()\n",
    "    for i in range(len(cl)):\n",
    "        if cl[i] == 1:\n",
    "            class1.append(i)\n",
    "        else:\n",
    "            class0.append(i)\n",
    "\n",
    "    for att_idx in range(n_attributes):  # calculate probabilities based on attributes\n",
    "        for obj_idx in class0:\n",
    "            if data[obj_idx][att_idx] == 1:\n",
    "                probabilities[att_idx][0] += 1\n",
    "        for obj_idx in class1:\n",
    "            if data[obj_idx][att_idx] == 1:\n",
    "                probabilities[att_idx][1] += 1\n",
    "        probabilities[att_idx][0] /= len(class0)\n",
    "        probabilities[att_idx][1] /= len(class1)\n",
    "\n",
    "    for i in range(n_attributes):  # calculate total probabilities\n",
    "        if obj[i] == 1:\n",
    "            p[1] *= probabilities[i][1]\n",
    "            p[0] *= probabilities[i][0]\n",
    "        else:\n",
    "            p[1] *= (1-probabilities[i][1])\n",
    "            p[0] *= (1-probabilities[i][0])\n",
    "\n",
    "    return p\n",
    "\n",
    "print(getNaiveBayesProbabilities([0,0,0,0], data, cl)) \n",
    "print(getNaiveBayesProbabilities([0,1,0,1], data, cl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6) Find objects which give a maximum probability for class \"0\" and \"1\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for class 0: (4, 0.1152)\n",
      "for class 1: (2, 0.144)\n"
     ]
    }
   ],
   "source": [
    "# we are not sure which objects to consider\n",
    "# do we limit ourselves to objects given in data?\n",
    "# do we consider set of all objects consisting of all possible binary vectors of length 4?\n",
    "# for now, we stick to \"data\", but it's simple to do the other thing as well\n",
    "\n",
    "# (idx, probability of belonging)\n",
    "max0data = (0, 0.0)\n",
    "max1data = (0, 0.0)\n",
    "for i in range(len(data)):\n",
    "    p = getNaiveBayesProbabilities(data[i], data, cl)\n",
    "    if p[0] > max0data[1]:\n",
    "        max0data = (i, p[0])\n",
    "    if p[1] > max1data[1]:\n",
    "        max1data = (i, p[1])\n",
    "\n",
    "print(\"for class 0: {}\".format(max0data))\n",
    "print(\"for class 1: {}\".format(max1data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: k-NN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) In common.py, there is defined a Euclidean distance function (cm.getEuclideanDistance(A, B)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'common' has no attribute 'getEuclideanDistance'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6496/2516940204.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#TEST\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetEuclideanDistance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetEuclideanDistance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'common' has no attribute 'getEuclideanDistance'"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "print(cm.getEuclideanDistance([0.0, 0.0], [0.0, 1.0]))\n",
    "print(cm.getEuclideanDistance([0.0, 0.0], [1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) Get test data set & display:<br>\n",
    "- DATA: data = matrix n x m, n = the number of objects, m = the number of attributes<br>\n",
    "- CLASSIFIED : classified objects, [[list of objects (indices) being assigned to the fist class],[... to the second class],...,[... to the last class]]; note that the classes do not impose any preference order\n",
    "- NOT_CLASSIFIED: list of objects (indices) being not classified\n",
    "\n",
    "IMPORTANT: plot $\\rightarrow$ black squares represent objects that are to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = cm.getTestDataSet()\n",
    "CLASSIFIED = cm.getTestClassified()\n",
    "NOT_CLASSIFIED = cm.getTestNotClassified()\n",
    "cm.displayDataSet(plt, DATA, CLASSIFIED, NOT_CLASSIFIED) #plt = plot package; see the imports above\n",
    "\n",
    "### CLASS 1 = BLUE\n",
    "### CLASS 2 = GREEN\n",
    "### CLASS 3 = RED\n",
    "### CLASS 4 = CYAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3) k-NN implementation: Complete the below function. Use the \"non-weighted\" variant of k-NN.  <br>\n",
    "(Advanced*) Add support to \"weighted\" variant of k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doKNN(K, DATA, CLASSIFIED, NOT_CLASSIFIED):\n",
    "    ### COPY INPUT CLASSIFIED OBJECTS \n",
    "    ### WHEN CLASSIFYING \"NOT_CLASSIFIED\" OBJECTS, UPDATE THE CL, i.e., ADD RESPECTIVE INDICES \n",
    "    CL = copy.deepcopy(CLASSIFIED)\n",
    "    ### For each index in NOT_CLASSIFIED, store here its corresponding (computed) class (in this exercise = 0, 1, or 2)\n",
    "    UPDATE = []\n",
    "    for IDX in NOT_CLASSIFIED:\n",
    "    # TODO\n",
    "    \n",
    "    ################\n",
    "    return CL\n",
    "#DISPLAY CLASSIFIED DATA\n",
    "NEW_CLASSIFIED = doKNN(2, DATA, CLASSIFIED, NOT_CLASSIFIED)\n",
    "cm.displayDataSet(plt, DATA, NEW_CLASSIFIED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) You are asked to evaluate the performance of k-NN (weighted variant) for different K. Firstly, run the below piece of code and analyze the data. **Which class do you think is the most difficult/easiest to describe/characterize?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_2 = cm.getTest2DataSet()\n",
    "CLASSIFIED_2 = cm.getTest2Classified()\n",
    "cm.displayDataSet(plt, DATA_2, CLASSIFIED_2)\n",
    "\n",
    "### CLASS 1 = BLUE\n",
    "### CLASS 2 = GREEN\n",
    "### CLASS 3 = RED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2) To evaluate the performance of k-NN, you can check how well it predicts classes for new (not known previously) objects. For this reason, assume that you \"do not know\" the true class assignments for some random subset of objects from the data set. Firstly, you can generate some permutation of indices from 0 to 249 (one per object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [i for i in range(250)]\n",
    "np.random.shuffle(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the below piece of code constructs temporary variables CLASSIFIED_TRAIN and NOT_CLASSIFIED_TEST. CLASSIFIED_TRAIN contains information on assignements for the first 75% random objects determined by \"idxs\", while NOT_CLASSIFIED_TEST contains the remianing \"not classified\" object indices. Run the code below and observe that 25% of some uniformly distributed data points are not classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIED_TRAIN = [[], [], []]\n",
    "NOT_CLASSIFIED_TEST = []\n",
    "LIM = int(len(DATA_2) * 0.5)\n",
    "for i in range(LIM):\n",
    "    if idxs[i] < 50: CLASSIFIED_TRAIN[0].append(idxs[i])\n",
    "    elif idxs[i] < 150: CLASSIFIED_TRAIN[1].append(idxs[i])\n",
    "    else: CLASSIFIED_TRAIN[2].append(idxs[i])\n",
    "for i in range(LIM, 250):\n",
    "    NOT_CLASSIFIED_TEST.append(idxs[i])\n",
    "cm.displayDataSet(plt, DATA_2, CLASSIFIED_TRAIN, NOT_CLASSIFIED_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3) Evaluate the performance of the weighted variant of k-NN for K = 1,3,5,7 and CLASSIFIED_TRAIN, NOT_CLASSIFIED_TEST data sets. For each K:<br>\n",
    "- for each class, compute how many times an object has been correctly/incorrectly classified. E.g., if 10 objects are associated with the first class \"BLUE\" but k-NN classified correctly only 4 of them, the result is 4 (or 6 in case of incorrect assignments stat);<br>\n",
    "- for each class, compute recall (number of correct assignments / total number of expected assignments, e.g., if 10 objects are associated with the first class \"BLUE\"  but k-NN classified correctly only 4 of them, recall for the first class is 40%);<br>  \n",
    "\n",
    "**Which class got the best/the worst recall. Why?**<br>\n",
    "**What is the best K. Are there any significant differences?**<br>\n",
    "You can use cm.displayCompareDataSet(plt, DATA_2, CLASSIFIED_2, CL) to check (display) which solutions were classified incorrectly. These are marked with squares. The background of these squares is appropriately selected according to the true (expected) assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLASS 1 = BLUE\n",
    "### CLASS 2 = GREEN\n",
    "### CLASS 3 = RED\n",
    "\n",
    "for K in [1,3,5,7]:\n",
    "    CL = doKNN(K, DATA_2, CLASSIFIED_TRAIN, NOT_CLASSIFIED_TEST)\n",
    "    print(K)\n",
    "    CORRECTLY = [0,0,0]\n",
    "    INCORRECTLY = [0,0,0]\n",
    "    RECALL = [0,0,0]\n",
    "    \n",
    "    #TODO\n",
    "            \n",
    "    ### PRINT STATS\n",
    "    print(CORRECTLY)\n",
    "    print(INCORRECTLY)\n",
    "    print(RECALL)\n",
    "    ### DISPLAY\n",
    "    cm.displayCompareDataSet(plt, DATA_2, CLASSIFIED_2, CL)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}